{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.2.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"Tensorflow version : {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moliere radius for lead (Wigmans2017, Appendix B)\n",
    "moliereRadius = tf.constant(16.0 / 3) # moliere radius in units of smallest irreducible unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label\n",
    "\n",
    "def get_seedlist(m, tseed):\n",
    "    \"\"\"Return seed list. Seeds are cells in the image (m) with a signal\n",
    "    above the seed threshold (tseed). Skimage.measure.label is used to \n",
    "    find connected components (neighbours are the 8 surrounding cells). \n",
    "    An entry consists of a proto-cluster id and the cell indices (i,j).\n",
    "    \n",
    "    Args:\n",
    "      m: A 2D `Tensor` with shape [height, width] of numeric type.\n",
    "      tseed: A scalar with same type as m.\n",
    "\n",
    "    Returns:\n",
    "      A `Tensor` with shape [None, 3] and type `int64`.\n",
    "    \"\"\"\n",
    "    mask = tf.math.greater(m, tseed)\n",
    "    f = lambda x: label(x, connectivity=2)\n",
    "    labels = tf.numpy_function(f, [mask], Tout=tf.int64)\n",
    "    sij = tf.where(labels)\n",
    "    sid = tf.expand_dims(tf.gather_nd(labels, sij), axis=1)\n",
    "    return tf.concat([sid, sij], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_seedlist(m, s):\n",
    "    \"\"\"Sorts seed list (s) in descending order of energy.\n",
    "    \n",
    "    Args:\n",
    "      m: A 2D `Tensor` with shape [height, width] of numeric type.\n",
    "      s: A `Tensor` with shape [None, 3] and type `int64`.\n",
    "      \n",
    "    Returns:\n",
    "      A `Tensor` with the same shape and type as s.\n",
    "    \"\"\"\n",
    "    sid, sij = tf.split(s, [1,2], axis=1)\n",
    "    seeds = tf.gather_nd(m, sij)\n",
    "    indices = tf.math.top_k(seeds, k=tf.shape(s)[0])[1]\n",
    "    return tf.gather(s, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_indices(indices, dense_shape):\n",
    "    \"\"\"Converts indices to boolean mask.\n",
    "    \n",
    "    Args:\n",
    "      indices: A `Tensor` of shape [None, 2] and type `int64`.\n",
    "      dense_type: A `Tensor` with shape [2,] and type `int64`.\n",
    "    \n",
    "    Returns:\n",
    "      A `Tensor` with shape [heigh, width] and type `bool`.\n",
    "    \"\"\"\n",
    "    values = tf.ones(tf.shape(indices)[0], tf.bool)\n",
    "    sp = tf.SparseTensor(indices, values, dense_shape)\n",
    "    return tf.sparse.to_dense(tf.sparse.reorder(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(idx, dense_shape,\n",
    "                   incl=tf.zeros([0,2], tf.int64),\n",
    "                   excl=tf.zeros([0,2], tf.int64)):\n",
    "    \"\"\"Return indices of 8 neighbors to the given index (idx).\n",
    "    \n",
    "    Args:\n",
    "      idx: A `Tensor` with shape [2,] and type `int64`.\n",
    "      dense_shape: A `Tensor` with shape [2,] and type `int64`.\n",
    "      incl: A `Tensor` with shape [None, 2] and the same type as idx.\n",
    "      excl: Same as for incl.\n",
    "      \n",
    "    Returns:\n",
    "      A `Tensor` with shape [None, 2] and type `int64`.\n",
    "    \"\"\"\n",
    "    # get indices of the 8 neighbours to idx\n",
    "    i, j = tf.unstack(idx)\n",
    "    irng = tf.range(i-1, i+2)\n",
    "    jrng = tf.range(j-1, j+2)\n",
    "    ii, jj = tf.meshgrid(irng, jrng, indexing='ij')\n",
    "    ii = tf.reshape(ii, [-1,1])\n",
    "    jj = tf.reshape(jj, [-1,1])\n",
    "    indices = tf.concat([ii, jj], axis=1)\n",
    "    mask_idx = tf.reduce_any(tf.not_equal(indices, idx), axis=1)\n",
    "    indices = tf.boolean_mask(indices, mask_idx)\n",
    "    \n",
    "    # handle boundaries\n",
    "    height, width = tf.unstack(dense_shape)\n",
    "    bound = tf.concat([[[height, -1]], [[-1, width]]], axis=0)\n",
    "    mask_bound = tf.not_equal(tf.expand_dims(indices, axis=-1), bound)\n",
    "    mask_bound = tf.reduce_all(mask_bound, axis=[1,2])\n",
    "    indices = tf.boolean_mask(indices, mask_bound)\n",
    "    \n",
    "    # get only indices in incl and remove indices in excl\n",
    "    mask_indices = get_mask_from_indices(indices, dense_shape)\n",
    "    mask_excl = get_mask_from_indices(excl, dense_shape)\n",
    "    mask_incl = get_mask_from_indices(incl, dense_shape)\n",
    "    mask_final = tf.logical_and(mask_indices, tf.logical_not(mask_excl))\n",
    "    pred = tf.equal(tf.size(incl), 0)\n",
    "    true_fn = lambda: mask_final\n",
    "    false_fn = lambda: tf.logical_and(mask_final, mask_incl)\n",
    "    mask_final = tf.cond(pred, true_fn, false_fn)\n",
    "    return tf.where(mask_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_adjacent_proto(nj, siid, m, p, s, l):\n",
    "    \"\"\"Merge adjacent proto-clusters.\"\"\"\n",
    "    siid = tf.reshape(siid, [-1,])\n",
    "    sid, sidx = tf.split(s, [1,2], axis=1)\n",
    "    lid, lidx = tf.split(l, [1,2], axis=1)\n",
    "    proto_id, proto_idx = tf.split(p, [1,2], axis=1)\n",
    "    dense_shape = tf.shape(m, tf.int64)\n",
    "    nnj = get_neighbours(nj, dense_shape, excl=sidx)\n",
    "    nnj_d = get_mask_from_indices(nnj, dense_shape)\n",
    "    proto_id = tf.reshape(proto_id, [-1,])\n",
    "    proto_sp = tf.SparseTensor(proto_idx, proto_id, dense_shape)\n",
    "    proto_d = tf.sparse.to_dense(tf.sparse.reorder(proto_sp))\n",
    "    mask_proto = tf.not_equal(proto_d, 0)\n",
    "    mask_neigh = tf.logical_and(nnj_d, mask_proto)\n",
    "    indices = tf.where(tf.logical_and(mask_neigh, tf.greater(m, tneighbour)))\n",
    "#     indices = tf.where(tf.logical_and(mask_neigh))\n",
    "    values = tf.gather_nd(proto_d, indices)\n",
    "    values = tf.boolean_mask(values, tf.not_equal(values, siid))\n",
    "    \n",
    "    # loop over values\n",
    "    k0 = tf.constant(0)\n",
    "    ck = lambda k, p, s, l: tf.less(k, tf.shape(values)[0])\n",
    "    def bk(k, p, s, l):\n",
    "        neigh_id = tf.gather(values, k)\n",
    "        pnew = tf.where(tf.equal(proto_d, neigh_id), siid*tf.ones_like(p), p)\n",
    "        snew = tf.where(tf.equal(sid, neigh_id), siid*tf.ones_like(s), s)\n",
    "        lnew = tf.where(tf.equal(lid, neigh_id), siid*tf.ones_like(l), l)\n",
    "        return [tf.add(k, 1), pnew, snew, lnew]\n",
    "    p, sid, lid = tf.while_loop(\n",
    "        ck, bk, loop_vars=[k0, proto_d, sid, lid],\n",
    "        shape_invariants=[k0.get_shape(), proto_d.get_shape(),\n",
    "                          sid.get_shape(), lid.get_shape()])[1:]               \n",
    "    indices = tf.where(tf.not_equal(p, 0))\n",
    "    values = tf.expand_dims(tf.gather_nd(p, indices), axis=1)\n",
    "    pnew = tf.concat([values, indices], axis=1)\n",
    "    snew = tf.concat([sid, sidx], axis=1)\n",
    "    lnew = tf.concat([lid, lidx], axis=1)\n",
    "    return pnew, snew, lnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_maker(j, m, p, s, l, n, siid):\n",
    "    \"\"\"Body of while loop in bi_maker.\"\"\"\n",
    "    nj = tf.gather(n, j)\n",
    "    nval = tf.gather_nd(m, nj)\n",
    "    nnew = tf.expand_dims(tf.concat([siid, nj], axis=0), axis=0)\n",
    "\n",
    "    def above_tneighbor(siid=siid):\n",
    "        # append cells to both proto-clusters and neighbor seed list\n",
    "        [pnew, snew, lnew] = merge_adjacent_proto(nj, siid, m, p, s, l)\n",
    "        pnew = tf.concat([pnew, nnew], axis=0)\n",
    "        lnew = tf.concat([lnew, nnew], axis=0)\n",
    "        return [pnew, snew, lnew]\n",
    "\n",
    "    def above_tcell():\n",
    "        # append cells only to proto-clusters\n",
    "        def true_fn():\n",
    "            return tf.concat([p, nnew], axis=0)\n",
    "        def false_fn(): return p\n",
    "        pnew = tf.cond(tf.greater(nval, tcell), true_fn, false_fn)\n",
    "        return [pnew, s, l]\n",
    "    \n",
    "    [pnew, snew, lnew] = tf.cond(tf.greater(nval, tneighbour),\n",
    "                                 true_fn=above_tneighbor,\n",
    "                                 false_fn=above_tcell)\n",
    "    \n",
    "    return tf.add(j, 1), m, pnew, snew, lnew, n, siid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_maker(i, m, p, s, l):\n",
    "    \"\"\"Body of while loop in find_neighbours_maker.\"\"\"\n",
    "    si = tf.gather(s, i)\n",
    "    \n",
    "    # find neighbours not in protolist\n",
    "    siid, siidx = tf.split(si, [1,2], axis=0)\n",
    "    pidx = tf.split(p, [1,2], axis=1)[1]\n",
    "    dense_shape = tf.shape(m, tf.int64)\n",
    "    n = get_neighbours(siidx, dense_shape, excl=pidx)\n",
    "        \n",
    "    # loop over neighbors\n",
    "    j0 = tf.constant(0)\n",
    "    cj = lambda j, m, p, s, l, n, siid: tf.less(j, tf.shape(n)[0])\n",
    "    p, s, l = tf.while_loop(\n",
    "        cj, bj_maker, loop_vars=[j0, m, p, s, l, n, siid],\n",
    "        shape_invariants=[j0.get_shape(), m.get_shape(), \n",
    "                          tf.TensorShape([None,3]),\n",
    "                          s.get_shape(), tf.TensorShape([None,3]),\n",
    "                          n.get_shape(), siid.get_shape()])[2:5]\n",
    "    \n",
    "    return tf.add(i, 1), m, p, s, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbours_maker(m, p, s):\n",
    "    \"\"\"One loop of finding neighbours.\n",
    "    \n",
    "    Args:\n",
    "      m: A 2D `Tensor` with shape [height, width] of numeric type.\n",
    "      p: A `Tensor` with shape [None, 3] and type `int64`.\n",
    "      s: Same as for p.\n",
    "      \n",
    "    Returns:\n",
    "      A tuple of Tensor objects (pnew, snew).\n",
    "      pnew: Same as for p.\n",
    "      snew: Same as for s.\n",
    "    \"\"\"\n",
    "    # sort current seed list in descending order\n",
    "    ssort = sort_seedlist(m, s)\n",
    "    \n",
    "    # loop over current seed list\n",
    "    i0 = tf.constant(0)\n",
    "    l0 = tf.zeros([0,3], tf.int64)\n",
    "    ci = lambda i, m, p, s, l: tf.less(i, tf.shape(ssort)[0])\n",
    "    pnew, snew, lnew = tf.while_loop(\n",
    "        ci, bi_maker, loop_vars=[i0, m, p, s, l0],\n",
    "        shape_invariants=[i0.get_shape(),\n",
    "                          m.get_shape(),\n",
    "                          tf.TensorShape([None,3]),\n",
    "                          s.get_shape(),\n",
    "                          tf.TensorShape([None,3])])[2:]\n",
    "    \n",
    "    # neighbor seed list becomes the new seed list\n",
    "    return pnew, lnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_maker(m, p):\n",
    "    \"\"\"Filter clusters by energy threshold\"\"\"\n",
    "    # convert to dense tensor\n",
    "    pid, pidx = tf.split(p, [1,2], axis=1)\n",
    "    dense_shape=tf.shape(m, out_type=tf.int64)\n",
    "    psp = tf.SparseTensor(pidx, tf.reshape(pid, [-1,]), dense_shape)\n",
    "    pd = tf.sparse.to_dense(tf.sparse.reorder(psp))\n",
    "\n",
    "    # loop over unique proto-cluster id's\n",
    "    u = tf.unique(tf.reshape(pid, [-1]), out_idx=pid.dtype)[0]\n",
    "    i0 = tf.constant(0)\n",
    "    c = lambda i, pdi: tf.less(i, tf.size(u))\n",
    "    def b(i, pdi):\n",
    "        ui = tf.gather(u, i)\n",
    "        mask = tf.equal(pdi, ui)\n",
    "        indices = tf.where(mask)\n",
    "        values = tf.gather_nd(m, indices)\n",
    "        Ei = tf.reduce_sum(values)\n",
    "        condition = tf.logical_and(mask, tf.less(Ei, tenergy))\n",
    "        pdi = tf.where(condition, tf.zeros_like(pdi), pdi)\n",
    "        return [tf.add(i, 1), pdi]\n",
    "    pdnew = tf.while_loop(c, b, loop_vars=[i0, pd],\n",
    "                          shape_invariants=[i0.get_shape(), pd.shape])[1]\n",
    "    \n",
    "    # convert to proto-list\n",
    "    pidxnew = tf.where(pdnew)\n",
    "    pidnew = tf.reshape(tf.gather_nd(pdnew, pidxnew), [-1,1])\n",
    "    pnew = tf.concat([pidnew, pidxnew], axis=1)\n",
    "    return pnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_maker(parsed, im):\n",
    "    \"\"\"Forms topological clusters from cells.\"\"\"\n",
    "    global tneighbour, tcell, tenergy\n",
    "    tneighbour = parsed['tneighbour']\n",
    "    tcell = parsed['tcell']\n",
    "    tenergy = parsed['tenergy']\n",
    "    \n",
    "    s = get_seedlist(parsed[im], parsed['tseed'])\n",
    "    \n",
    "    # finding neighbours recursively until current seed list is empty\n",
    "    c = lambda pi, si: tf.not_equal(tf.size(si), 0)\n",
    "    b = lambda pi, si: find_neighbours_maker(parsed[im], pi, si)\n",
    "    pnew = tf.while_loop(\n",
    "        c, b, loop_vars=[s, s],\n",
    "        shape_invariants=[tf.TensorShape([None,3]), \n",
    "                          tf.TensorShape([None,3])])[0]\n",
    "    \n",
    "    parsed = parsed.copy()\n",
    "    parsed['proto'] = finalize_maker(parsed[im], pnew)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster maker is sufficient for isolated signals, but not for overlapping showers. If individual particles form local maxima they may still be separable. Acting on the clusters resulting from the cluster maker, this is what the cluster splitter does in the followning steps:\n",
    "\n",
    "* **Finding local maxima**: a local maxima is defined as a cell with: a) $E>t_\\text{locmax}$, b) energy greater than that of any neighboring cell, and c) number of neighboring cells withing the parent cluster $N>t_\\text{num}$ (default is $\\geq4$). Each local maximum forms a cluster and parent clusters without any local maximum cell will not be split.\n",
    "* **Finding neighbors**: the local maxima now becomes the initial seed list much like in cluster maker, except that only cells originally clustered are used, without thresholding and merging. Instead of merging, shared cells are added to a shared cell list to be handled separately.\n",
    "* **Shared cells**: the shared cell list is expanded iteratively adding neighbors from the originally clustered cells not yet assigned to any proto-cluster. Each of these are then added to the two adjoining proto-clusters with the weights $w_1=\\frac{E_{1}}{E_{1}+rE_{2}}, w_{2}=1-w_{1}, r=\\exp(d_{1}-d_{2})$, where $E_{1,2}$ are the energies of the two proto-clusters and $d_{1,2}$ are the distances of the shared cell to the proto-cluster centroids in units of a typical em shower scale.\n",
    "* **Finalize**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding local maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_local_maxima(m, p):\n",
    "    \"\"\"Find local maxima cells by looping over protolist.\n",
    "    m: filtered image, x: local maxima\"\"\"\n",
    "    \n",
    "    # loop over protolist\n",
    "    dense_shape = tf.shape(m, tf.int64)\n",
    "    i0 = tf.constant(0)\n",
    "    x = tf.zeros([0,3], p.dtype)\n",
    "    c = lambda i, p, x: tf.less(i, tf.shape(p)[0])\n",
    "    def b(i, p, x):\n",
    "        pi = tf.gather(p, i)\n",
    "        piid, piidx = tf.split(pi, [1,2], axis=0)\n",
    "        pval = tf.gather_nd(m, piidx)\n",
    "        midx = tf.where(tf.greater(m, 0))\n",
    "        nidx = get_neighbours(piidx, dense_shape, incl=midx)\n",
    "        m_val = tf.gather_nd(m, nidx)\n",
    "#         m_val = tf.boolean_mask(m_val, tf.greater(m_val, 0))\n",
    "\n",
    "        pred = tf.logical_and(tf.logical_and(tf.greater(pval, tlocmax), \n",
    "                                      tf.greater(pval, tf.math.reduce_max(m_val))),\n",
    "                                      tf.greater(tf.size(m_val), tnum))\n",
    "        r = tf.cond(pred, lambda: tf.expand_dims(pi, axis=0), lambda: tf.zeros([0,3], pi.dtype))\n",
    "        \n",
    "        return [tf.add(i, 1), p, tf.concat([x, r], axis=0)]\n",
    "    \n",
    "    x = tf.while_loop(\n",
    "        c, b, loop_vars=[i0, p, x], \n",
    "        shape_invariants=[i0.get_shape(), p.get_shape(), tf.TensorShape([None,3])])[2]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_shared(m, nj, p, siid):\n",
    "    dense_shape = tf.shape(m, out_type=tf.int64)\n",
    "    nnj = get_neighbours(nj, dense_shape)\n",
    "    proto_id, proto_idx = tf.split(p, num_or_size_splits=[1,2], axis=1)\n",
    "    \n",
    "    nnj_d = get_mask_from_indices(nnj, dense_shape)\n",
    "    proto_sp = tf.SparseTensor(proto_idx, tf.reshape(proto_id, [-1,]), dense_shape)\n",
    "    proto_d = tf.sparse.to_dense(tf.sparse.reorder(proto_sp))\n",
    "    indices = tf.where(tf.logical_and(nnj_d, tf.not_equal(proto_d, 0)))\n",
    "    values = tf.gather_nd(proto_d, indices)\n",
    "    y, idx = tf.unique(values)\n",
    "    return [y, tf.equal(tf.size(y), 2)]\n",
    "\n",
    "def bj_splitter(j, m, p, s, l, o, n, siid):\n",
    "    \"\"\"Body of while loop over neighbors\"\"\"\n",
    "    nj = tf.gather(n, j)\n",
    "    \n",
    "    ids, pred = is_shared(m, nj, p, siid)\n",
    "    \n",
    "    def true_fn():\n",
    "        nnew = tf.expand_dims(tf.concat([ids, nj], axis=0), axis=0)\n",
    "        onew = tf.concat([o, nnew], axis=0)\n",
    "        return [p, s, l, onew]\n",
    "    def false_fn():\n",
    "        nnew = tf.expand_dims(tf.concat([siid, nj], axis=0), axis=0)\n",
    "        pnew = tf.concat([p, nnew], axis=0)\n",
    "        lnew = tf.concat([l, nnew], axis=0)\n",
    "        return [pnew, s, lnew, o]\n",
    "\n",
    "    [pnew, snew, lnew, onew] = tf.cond(pred, true_fn, false_fn)\n",
    "        \n",
    "    return [tf.add(j, 1), m, pnew, snew, lnew, onew, n, siid]\n",
    "\n",
    "def bi_splitter(i, m, p, s, l, o):\n",
    "    \"\"\"Body of while loop over current seed list\"\"\"\n",
    "    si = tf.gather(s, i)\n",
    "    \n",
    "    # find neighbours not in protolist and only include cells originally clustered\n",
    "    siid, siidx = tf.split(si, [1,2], axis=0)\n",
    "    pidx = tf.split(p, [1,2], axis=1)[1]\n",
    "    oidx = tf.split(o, num_or_size_splits=[2,2], axis=1)[1]\n",
    "    midx = tf.where(tf.greater(m, 0))\n",
    "    dense_shape = tf.shape(m, tf.int64)\n",
    "    n = get_neighbours(siidx, dense_shape, incl=midx, excl=tf.concat([pidx, oidx], axis=0))\n",
    "    \n",
    "    # loop over neighbors\n",
    "    j0 = tf.constant(0)\n",
    "    cj = lambda j, m, p, s, l, o, n, siid: tf.less(j, tf.shape(n)[0])\n",
    "    _, m, p, s, l, o, _, _ = tf.while_loop(\n",
    "        cj, bj_splitter, loop_vars=[j0, m, p, s, l, o, n, siid],\n",
    "        shape_invariants=[j0.get_shape(), m.get_shape(), tf.TensorShape([None,3]),\n",
    "                          s.get_shape(), tf.TensorShape([None,3]), tf.TensorShape([None,4]),\n",
    "                          n.get_shape(), siid.get_shape()])\n",
    "    \n",
    "    return [tf.add(i, 1), m, p, s, l, o]\n",
    "\n",
    "def finding_neighbors_splitter(m, p, s, o):\n",
    "    \"\"\"Finding neighbors recursively until current seed list is empty.\n",
    "    m: image, p: protolist, s: seedlist, l: neighlist, n: neighbor, o: sharedlist\"\"\"\n",
    "    # sort current seed list in descending order\n",
    "    ssort = sort_seedlist(m, s)\n",
    "    \n",
    "    # loop over current seed list\n",
    "    i0 = tf.constant(0)\n",
    "    l0 = tf.zeros([0,3], tf.int64)\n",
    "    ci = lambda i, m, p, s, l, o: tf.less(i, tf.shape(ssort)[0])\n",
    "    _, m, pnew, s, lnew, onew = tf.while_loop(\n",
    "        ci, bi_splitter, loop_vars=[i0, m, p, s, l0, o],\n",
    "        shape_invariants=[i0.get_shape(), m.get_shape(), \n",
    "                          tf.TensorShape([None,3]), s.get_shape(),\n",
    "                          tf.TensorShape([None,3]), tf.TensorShape([None,4])])\n",
    "    \n",
    "    # neighbor seed list becomes the new seed list\n",
    "    return pnew, lnew, onew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_shared(j, p, l, n, siid):\n",
    "    \"\"\"Body of while loop over neighbors\"\"\"\n",
    "    nj = tf.gather(n, j)\n",
    "    nnew = tf.expand_dims(tf.concat([siid, nj], axis=0), axis=0)\n",
    "    pnew = tf.concat([p, nnew], axis=0)\n",
    "    lnew = tf.concat([l, nnew], axis=0)\n",
    "    return [tf.add(j, 1), pnew, lnew, n, siid]\n",
    "\n",
    "def bi_shared(i, m, p, s, l):\n",
    "    \"\"\"Body of while loop over current seed list\"\"\"\n",
    "    si = tf.gather(s, i)\n",
    "    \n",
    "    # find neighbours not in protolist and only include cells originally clustered\n",
    "    siid, siidx = tf.split(si, num_or_size_splits=[2,2], axis=0)\n",
    "    _, pidx = tf.split(p, num_or_size_splits=[2,2], axis=1)\n",
    "    midx = tf.where(m)\n",
    "    dense_shape = tf.shape(m, out_type=tf.int64)\n",
    "    n = get_neighbours(siidx, dense_shape, incl=midx, excl=pidx)\n",
    "    \n",
    "    # loop over neighbors\n",
    "    j0 = tf.constant(0)\n",
    "    cj = lambda j, p, l, n, siid: tf.less(j, tf.shape(n)[0])\n",
    "    _, p, l, _, _ = tf.while_loop(\n",
    "        cj, bj_shared, loop_vars=[j0, p, l, n, siid],\n",
    "        shape_invariants=[j0.get_shape(), tf.TensorShape([None, 4]), tf.TensorShape([None, 4]),\n",
    "                          n.get_shape(), siid.get_shape()])\n",
    "    \n",
    "    return [tf.add(i, 1), m, p, s, l]\n",
    "\n",
    "def expand_sharedlist(m, p, s):\n",
    "    # loop over current seed list\n",
    "    i0, l0 = tf.constant(0), tf.zeros([0,4], s.dtype)\n",
    "    ci = lambda i, m, p, s, l: tf.less(i, tf.shape(s)[0])\n",
    "    _, _, pnew, s, lnew = tf.while_loop(\n",
    "        ci, bi_shared, loop_vars=[i0, m, p, s, l0],\n",
    "        shape_invariants=[i0.get_shape(), m.get_shape(), tf.TensorShape([None,4]), \n",
    "                          s.get_shape(), tf.TensorShape([None,4])])\n",
    "    \n",
    "    # neighbor seed list becomes the new seed list\n",
    "    return pnew, lnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize cluster splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_splitter(m, p, o):\n",
    "    pid, pidx = tf.split(p, num_or_size_splits=[1,2], axis=1)\n",
    "    pval = tf.gather_nd(m, pidx)\n",
    "    dense_shape = tf.shape(m, out_type=tf.int64)\n",
    "    psp = tf.SparseTensor(pidx, tf.reshape(pid, [-1,]), dense_shape)\n",
    "    pd = tf.sparse.to_dense(tf.sparse.reorder(psp))\n",
    "    pnew = tf.concat([tf.cast(pid, pval.dtype), tf.cast(pidx, pval.dtype), tf.expand_dims(pval, axis=1)], axis=1)\n",
    "    \n",
    "    y = tf.unique(tf.reshape(pid, [-1,]))[0]\n",
    "#     print(\"unique pid\", y)\n",
    "    \n",
    "    i0, com0, E0 = tf.constant(0), tf.zeros([0,2], m.dtype), tf.zeros([0, 1], m.dtype)\n",
    "    ci = lambda i, com, E: tf.less(i, tf.size(y))\n",
    "    def bi(i, com, E):\n",
    "        yi = tf.gather(y, i)\n",
    "        indices = tf.where(tf.equal(pd, yi))\n",
    "        values = tf.expand_dims(tf.gather_nd(m, indices), axis=1)\n",
    "        indices = tf.cast(indices, values.dtype)\n",
    "        comi = tf.reduce_sum(tf.multiply(indices, values), axis=0)\n",
    "        Ei = tf.reduce_sum(values)\n",
    "        comi = tf.divide(comi, Ei)\n",
    "        comi = tf.expand_dims(comi, axis=0)\n",
    "        Ei = tf.reshape(Ei, [-1,1])\n",
    "        return [tf.add(i, 1), tf.concat([com, comi], axis=0), tf.concat([E, Ei], axis=0)]\n",
    "    _, com, E = tf.while_loop(\n",
    "        ci, bi, loop_vars=[i0, com0, E0],\n",
    "        shape_invariants=[i0.get_shape(), tf.TensorShape([None,2]), tf.TensorShape([None, 1])])\n",
    "#     print(com, E)\n",
    "    \n",
    "    j0, l0 = tf.constant(0), tf.zeros([0, 4], m.dtype)\n",
    "    cj = lambda j, l: tf.less(j, tf.shape(o)[0])\n",
    "    def bj(j, l):\n",
    "        oj = tf.gather(o, j)\n",
    "        a, b, ojidx = tf.split(oj, num_or_size_splits=[1,1,2], axis=0)\n",
    "        oval = tf.expand_dims(tf.gather_nd(m, ojidx), axis=0)\n",
    "        ojidx = tf.cast(ojidx, com.dtype)\n",
    "        amask = tf.equal(y, a)\n",
    "        bmask = tf.equal(y, b)\n",
    "        acom = tf.reshape(tf.boolean_mask(com, amask), [-1,])\n",
    "        bcom = tf.reshape(tf.boolean_mask(com, bmask), [-1,])\n",
    "        d1 = tf.sqrt(tf.reduce_sum(tf.math.squared_difference(ojidx, acom))) / moliereRadius\n",
    "        d2 = tf.sqrt(tf.reduce_sum(tf.math.squared_difference(ojidx, bcom))) / moliereRadius\n",
    "        # missing in units of !\n",
    "        r = tf.exp(tf.subtract(d1, d2))\n",
    "        E1 = tf.boolean_mask(E, amask)\n",
    "        E2 = tf.boolean_mask(E, bmask)\n",
    "        w1 = tf.reshape(tf.divide(E1, tf.add(E1, tf.multiply(r, E2))), [-1,])\n",
    "        w2 = tf.subtract(1., w1)\n",
    "        la = tf.expand_dims(tf.concat([tf.cast(a, com.dtype), ojidx, tf.multiply(w1, oval)], axis=0), axis=0)\n",
    "        lb = tf.expand_dims(tf.concat([tf.cast(b, com.dtype), ojidx, tf.multiply(w2, oval)], axis=0), axis=0)\n",
    "        return [tf.add(j, 1), tf.concat([l, la, lb], axis=0)]\n",
    "    _, lnew = tf.while_loop(\n",
    "        cj, bj, loop_vars=[j0, l0],\n",
    "        shape_invariants=[j0.get_shape(), tf.TensorShape([None, 4])])\n",
    "\n",
    "    pnew = tf.concat([pnew, lnew], axis=0)\n",
    "\n",
    "    # sort in descending order in energy \n",
    "    indices = tf.reshape(tf.cast(tf.math.top_k(tf.reshape(E, [1, -1]), k=tf.shape(E)[0])[1], o.dtype), [-1,])\n",
    "#     print(y, indices)\n",
    "    \n",
    "    k0, p0 = tf.constant(0), tf.zeros([0, 4], pnew.dtype)\n",
    "    ck = lambda k, p: tf.less(k, tf.shape(pnew)[0])\n",
    "    def bk(k, p):\n",
    "        pk = tf.gather(pnew, k)\n",
    "        pkid, pkidx = tf.split(pk, num_or_size_splits=[1,3], axis=0)\n",
    "        yidx = tf.reshape(tf.where(tf.equal(y, tf.cast(pkid, y.dtype))), [-1,])\n",
    "        pkid = tf.cast(tf.add(tf.where(tf.equal(indices, yidx)), 1), pkidx.dtype)\n",
    "        pknew = tf.concat([pkid, tf.expand_dims(pkidx, axis=0)], axis=1)\n",
    "        return [tf.add(k, 1), tf.concat([p, pknew], axis=0)]\n",
    "    pnew = tf.while_loop(ck, bk, loop_vars=[k0, p0],\n",
    "                        shape_invariants=[k0.get_shape(), tf.TensorShape([None, 4])])[1]\n",
    "    \n",
    "    return pnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_splitter(parsed, im):\n",
    "    global tlocmax, tnum\n",
    "    proto = parsed['proto']\n",
    "    image = parsed[im]\n",
    "    tlocmax = parsed['tlocmax']\n",
    "    tnum = parsed['tnum']\n",
    "    \n",
    "    # filter image with protolist\n",
    "    proto_id, proto_idx = tf.split(proto, [1,2], axis=1)\n",
    "    proto_val = tf.gather_nd(image, proto_idx)\n",
    "    dense_shape = tf.shape(image, out_type=tf.int64)\n",
    "    image_sp = tf.SparseTensor(proto_idx, proto_val, dense_shape)\n",
    "    image_d = tf.sparse.to_dense(tf.sparse.reorder(image_sp))\n",
    "    \n",
    "    locmax = finding_local_maxima(image_d, proto)\n",
    "    tf.print(\"locmax\", locmax)\n",
    "    \n",
    "    # create seed list from local maxima\n",
    "    locmax_idx = tf.split(locmax, [1,2], axis=1)[1]\n",
    "    proto_sp = tf.SparseTensor(proto_idx, tf.reshape(proto_id, [-1,]), dense_shape)\n",
    "    proto_d = tf.sparse.to_dense(tf.sparse.reorder(proto_sp))\n",
    "    start = tf.add(tf.reduce_max(proto_id), 1)\n",
    "    limit = start + tf.shape(locmax, out_type=proto_id.dtype)[0]\n",
    "    locmax_id = tf.expand_dims(tf.range(start, limit, dtype=locmax.dtype), axis=1)\n",
    "    seedlist = tf.concat([locmax_id, locmax_idx], axis=1)\n",
    "    tf.print(\"seedlist\", seedlist)\n",
    "    \n",
    "    sharedlist = tf.zeros([0,4], tf.int64)\n",
    "    \n",
    "    # finding neighbours recursively until current seed list is empty\n",
    "    c = lambda pi, si, oi: tf.not_equal(tf.size(si), 0)\n",
    "    b = lambda pi, si, oi: finding_neighbors_splitter(image_d, pi, si, oi)\n",
    "    [protolist, seedlist, sharedlist] = tf.while_loop(c, b, loop_vars=[seedlist, seedlist, sharedlist],\n",
    "                         shape_invariants=[tf.TensorShape([None, 3]), \n",
    "                                           tf.TensorShape([None, 3]),\n",
    "                                           tf.TensorShape([None, 4])])\n",
    "    \n",
    "#     [protolist, seedlist, sharedlist] = finding_neighbors_splitter(image_d, seedlist, seedlist, sharedlist)\n",
    "    \n",
    "    # originally clustered cells not in protolist\n",
    "    protolist_id, protolist_idx = tf.split(protolist, [1,2], axis=1)\n",
    "    protolist_sp = tf.SparseTensor(protolist_idx, tf.reshape(protolist_id, [-1,]), dense_shape)\n",
    "    protolist_d = tf.sparse.to_dense(tf.sparse.reorder(protolist_sp))\n",
    "    mask = tf.logical_and(tf.cast(proto_d, tf.bool), tf.logical_not(tf.cast(protolist_d, tf.bool)))\n",
    "    \n",
    "    c = lambda pi, si: tf.not_equal(tf.size(si), 0)\n",
    "    b = lambda pi, si: expand_sharedlist(mask, pi, si)\n",
    "    sharedlist = tf.while_loop(c, b, loop_vars=[sharedlist, sharedlist],\n",
    "                         shape_invariants=[tf.TensorShape([None, 4]), \n",
    "                                           tf.TensorShape([None, 4])])[0]\n",
    "    \n",
    "    \n",
    "# #     sharedlist = expand_sharedlist(mask, sharedlist, sharedlist)\n",
    "    \n",
    "    # add parent clusters without a local maximum\n",
    "    mask = tf.logical_and(tf.cast(proto_d, tf.bool), tf.cast(protolist_d, tf.bool))\n",
    "    proto_y = tf.expand_dims(tf.unique(tf.reshape(proto_id, [-1,]))[0], axis=0)\n",
    "    proto_masked_y = tf.expand_dims(tf.unique(tf.boolean_mask(proto_d, mask))[0], axis=0)\n",
    "    other = tf.sparse.to_dense(tf.sets.difference(proto_y, proto_masked_y))\n",
    "\n",
    "    other = tf.reshape(other, [-1,1])\n",
    "    \n",
    "#     if tf.not_equal(tf.size(other), 0):\n",
    "    i = tf.constant(0)\n",
    "    c = lambda i, p: tf.less(i, tf.shape(other)[0])\n",
    "    def b(i, p): \n",
    "        otheri = tf.gather(other, i)\n",
    "        p = tf.where(tf.equal(proto_d, otheri), proto_d, p) \n",
    "        return [tf.add(i, 1), p]\n",
    "    protolist_d = tf.while_loop(c, b, [i, protolist_d])[1]\n",
    "    pidx = tf.where(protolist_d)\n",
    "    pid = tf.expand_dims(tf.gather_nd(protolist_d, pidx), axis=1)\n",
    "    protolist = tf.concat([pid, pidx], axis=1)\n",
    "    \n",
    "    cluster = finalize_splitter(image_d, protolist, sharedlist)\n",
    "        \n",
    "    parsed['cluster'] = cluster\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminating variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_features(parsed):\n",
    "    cluster = parsed['cluster']\n",
    "    \n",
    "    cid, cidx, cval = tf.split(cluster, [1,2,1], axis=1)    \n",
    "    y = tf.unique(tf.reshape(cid, [-1,]))[0]\n",
    "#     print(\"unique pid\", y)\n",
    "    \n",
    "    i0 = tf.constant(0)\n",
    "    z0 = tf.zeros([0,5], cluster.dtype)\n",
    "    c = lambda i, z: tf.less(i, tf.size(y))\n",
    "    def bi(i, z):\n",
    "        yi = tf.gather(y, i)\n",
    "        cid_indices = tf.where(tf.equal(tf.reshape(cid, [-1,]), yi))\n",
    "        values = tf.gather_nd(cval, cid_indices)\n",
    "        indices = tf.reshape(tf.gather(cidx, cid_indices), [-1,2])\n",
    "        com = tf.reduce_sum(tf.multiply(indices, values), axis=0, keepdims=True)\n",
    "        com /= tf.reduce_sum(values)\n",
    "    \n",
    "        signal_sum = tf.reduce_sum(values, keepdims=True)\n",
    "        signal_max = tf.cond(tf.greater(signal_sum, 0.), lambda: tf.reduce_max(values), lambda: tf.constant([[0.]]))\n",
    "        signal_hot = tf.cond(tf.greater(signal_sum, 0.), lambda: signal_max/signal_sum, lambda: tf.constant([[0.]]))\n",
    "        signal_rad = tf.sqrt(tf.reduce_sum(tf.pow(indices-com, 2), axis=1, keepdims=True))\n",
    "        signal_rad_mean = tf.cond(tf.greater(signal_sum, 0.), \n",
    "                             lambda: tf.reduce_sum(tf.multiply(signal_rad, values), axis=0, keepdims=True)/tf.reduce_sum(values),\n",
    "                             lambda: tf.constant([[0.]]))\n",
    "        \n",
    "        zi = tf.concat([com, signal_sum, signal_rad_mean, signal_hot], axis=1)\n",
    "        \n",
    "        return [tf.add(i, 1), tf.concat([z, zi], axis=0)]\n",
    "    z = tf.while_loop(\n",
    "        c, bi, loop_vars=[i0, z0],\n",
    "        shape_invariants=[i0.get_shape(), tf.TensorShape([None,5])])[1]\n",
    "    parsed['feature'] = z\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# def serialize_example(f0,f1,f2,f3,f4,f5,f6,f7,f8):\n",
    "def serialize_example(f0,f1,f2,f3,f4,f5):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'eventId': _int64_feature(f0),\n",
    "      'cluster_comi': _float_feature(f1),\n",
    "      'cluster_comj': _float_feature(f2),\n",
    "      'S_sum': _float_feature(f3),\n",
    "      'S_rad_mean': _float_feature(f4),\n",
    "      'S_hot': _float_feature(f5),\n",
    "#       'C_sum': _float_feature(f6),\n",
    "#       'C_rad_mean': _float_feature(f7),\n",
    "#       'C_hot': _float_feature(f8)\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def tf_serialize_example(example):\n",
    "    f0 = example['eventId']\n",
    "#     f1,f2,f3,f4,f5,f6,f7,f8 = tf.split(example['feature'], num_or_size_splits=8, axis=1)\n",
    "    f1,f2,f3,f4,f5 = tf.split(example['feature'], num_or_size_splits=5, axis=1)\n",
    "#     tf_string = tf.py_function(serialize_example, (f0,f1,f2,f3,f4,f5,f6,f7,f8), tf.string)\n",
    "    tf_string = tf.py_function(serialize_example, (f0,f1,f2,f3,f4,f5), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
