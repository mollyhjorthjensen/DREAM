{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "assert tf.__version__=='2.2.0-rc1'\n",
    "# assert tf.test.is_gpu_available()\n",
    "K.clear_session()\n",
    "%load_ext tensorboard\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moliere radius for lead (Wigmans2017, Appendix B)\n",
    "moliereRadius = 16.0 / 3 # moliere radius in units of smallest irreducible unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ''\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 284\n",
    "WIDTH = 284\n",
    "TENERGY = 500\n",
    "TLOCMAX = 350\n",
    "TNUMBER = 3\n",
    "# good values to test if recursive function works\n",
    "TSEED = tf.constant(100, tf.float32)  # tf.constant([300.], tf.float32)\n",
    "TNEIGHBOR = tf.constant(50., tf.float32)\n",
    "TCELL = tf.constant(10., tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'eventId': tf.io.FixedLenFeature([1], tf.int64, default_value=0),\n",
    "    'image': tf.io.FixedLenFeature([3], tf.string, default_value=[\"\",]*3)\n",
    "}\n",
    "\n",
    "feature_shape = {\n",
    "    'eventId': tf.TensorShape([1,]),\n",
    "    'image': tf.TensorShape([568, 568, 2])\n",
    "}\n",
    "\n",
    "def parser_fn(proto):\n",
    "    serialized = tf.io.parse_single_example(proto, feature_description)\n",
    "    deserialized = {k: (tf.sparse.to_dense(sparse_ops.deserialize_sparse(v, K.floatx()))\n",
    "                        if k != 'eventId' else v) for k,v in serialized.items()}\n",
    "    [deserialized[k].set_shape(feature_shape[k]) for k in deserialized.keys()]\n",
    "    x = deserialized['image']\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    x = K.pool2d(x, pool_size=(2, 2), strides=(2, 2), pool_mode='avg')\n",
    "    # sum instead of avg\n",
    "    x = 4.*x\n",
    "    deserialized['image'] = tf.squeeze(x)\n",
    "    \n",
    "    return deserialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: {eventId: (1,), image: (284, 284, 2)}, types: {eventId: tf.int64, image: tf.float32}>\n"
     ]
    }
   ],
   "source": [
    "filename = [os.path.join(DATA_DIR, 'B4.tfrecord')]\n",
    "dataset = tf.data.TFRecordDataset(filename, compression_type='GZIP', buffer_size=BUFFER_SIZE)\n",
    "dataset = dataset.map(parser_fn)\n",
    "# dataset = dataset.take(1).map(cluster_maker)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n"
     ]
    }
   ],
   "source": [
    "cols = ['eventId', 'cluster_comi', 'cluster_comj', 'S_sum', 'S_rad_mean', 'S_hot', 'C_sum', 'C_rad_mean', 'C_hot']\n",
    "pdf = pd.DataFrame(columns=cols)\n",
    "i = 0\n",
    "for parsed in dataset:\n",
    "    print(i)\n",
    "    output = cluster_maker(parsed)\n",
    "    output2 = cluster_splitter(output)\n",
    "    output3 = scalar_features(output2)\n",
    "    s = parsed['scalar'].numpy()\n",
    "    eventId = parsed['eventId'].numpy()\n",
    "    eventId_rep = np.tile(eventId, [s.shape[0], 1])\n",
    "    arr = np.hstack((eventId_rep, s))\n",
    "    pdfi = pd.DataFrame(arr, columns=cols)\n",
    "    pdf = pdf.append(pdfi)\n",
    "    i += 1\n",
    "pdf.reset_index(drop=True, inplace=True)\n",
    "pdf.eventId = pdf.eventId.astype(int)\n",
    "pdf.to_csv('clustering.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups neighboring cells of significant energy into clusters with a variable number of cells (vs sliding-window algorithm). Cluster growth starts at seed cells, i.e. cells with significance above $t_\\text{seed}$ (high). Neighboring cells become new seeds if their significance are above $t_\\text{neighbor}$ (medium). If a neighboring cell has significance only above $t_\\text{cell}$ (low) it is simply added to the cluster without functioning as a seed. The low threshold means shower tails are not discarded, the other thresholds supress noise. This algorithm consists of two steps: the cluster maker and the cluster splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forms topological clusters from cells and consists of the following steps:\n",
    "\n",
    "* **Finding seeds**: seed list consists of cells with significance above $t_\\text{seed}$. Neighboring seed cells form initial proto-clusters (vs paper where each seed cell forms a proto-cluster) - possibly look at later. NB missing noise!\n",
    "* **Finding neighbors**: current seed list ordered in descending order in significance. Consider neighboring cells of each seed cell in turn: a) if significance above $t_\\text{neighbor}$ it is added to the neighbor seed list and adjacent proto-cluster; if adjacent to multiple proto-clusters, these are merged. b) if significance only above $t_\\text{cell}$ cell included only in first adjacent proto-cluster (which has the more significant neighbor due to the sorting), not in the neighbor seed list (it is unclear from the paper if the latter is true). When all seed cells have been processed, the neighbor seed list becomes the new seed list. This procedure of neighbor finding is repeated until the seed list is empty.\n",
    "* **Finalize**: resulting proto-clusters are sorted in descending order in $E_T$ and filtered $E_T>t_\\text{energy}$ - does $E_T$ refer to transverse energy or total energy or total energy above threshold?  \n",
    "\n",
    "\"Neighboring\" is here defined as the eight surronding cells within the same channel and overlapping cells in adjacent channels. The scintillating and Cerenkov signals are treated as two different channels, consequently, each cell has nine neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.measure import label\n",
    "import skimage\n",
    "from skimage import measure\n",
    "\n",
    "f = lambda x: skimage.measure.label(x, connectivity=2)\n",
    "\n",
    "def finding_seeds(image):\n",
    "    \"\"\"Finding seeds and create seed list\"\"\"\n",
    "    seed_mask = tf.math.greater(image, TSEED)\n",
    "    labels = tf.numpy_function(f, [seed_mask], Tout=tf.int64)\n",
    "    seed_idx = tf.where(tf.not_equal(labels, 0))\n",
    "    seed_id = tf.expand_dims(tf.gather_nd(labels, seed_idx), axis=1)\n",
    "    seed_list = tf.concat([seed_id, seed_idx], axis=-1)\n",
    "    return seed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_seedlist(seed_list, seed_len, image):\n",
    "    \"\"\"Sorts current seed list in descending order\"\"\"\n",
    "    seed_id, seed_idx = tf.split(seed_list, num_or_size_splits=[1,3], axis=1)\n",
    "    seeds = tf.gather_nd(image, seed_idx)\n",
    "    values, indices = tf.math.top_k(seeds, k=seed_len)\n",
    "    return tf.gather(seed_list, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(index, excl=tf.zeros([0, 3], tf.int64), incl=tf.zeros([0, 3], tf.int64)):\n",
    "    \"\"\"8-1 connectivity\"\"\"\n",
    "    \n",
    "    # indices of 3x3x1 centred at (i,j,k)\n",
    "    i, j, k = tf.unstack(index)\n",
    "    ii, jj = tf.meshgrid(tf.range(i-1, i+2), tf.range(j-1, j+2), indexing='ij')\n",
    "    ii = tf.reshape(ii, [-1, 1])\n",
    "    jj = tf.reshape(jj, [-1, 1])\n",
    "    kk = tf.repeat(tf.reshape(k, (1, 1)), repeats=[9], axis=0)\n",
    "    a = tf.concat([ii, jj, kk], axis=-1)\n",
    "    \n",
    "    # append index (i,j) in other k channel\n",
    "    a = tf.concat([a, [[i, j, tf.math.abs(k-1)]]], axis=0)\n",
    "    \n",
    "    # remove centre point (i,j,k)\n",
    "    mask_centre = tf.reduce_any(tf.not_equal(a, index), axis=-1)\n",
    "    \n",
    "    # remove points outside image\n",
    "    b = tf.expand_dims(a, axis=-1)\n",
    "    c = tf.transpose(tf.constant([[HEIGHT,-1, -1], [-1,WIDTH,-1]], dtype=b.dtype))\n",
    "    mask_boundary = tf.reduce_all(tf.not_equal(b, c), axis=[-2,-1])\n",
    "    mask = tf.logical_and(mask_centre, mask_boundary)\n",
    "    a = tf.boolean_mask(a, mask)\n",
    "    \n",
    "    # remove points in excl and only consider points in incl\n",
    "    a_sp = tf.SparseTensor(a, tf.ones(tf.shape(a)[0], dtype=tf.bool), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    a_d = tf.sparse.to_dense(tf.sparse.reorder(a_sp))\n",
    "    excl_sp = tf.SparseTensor(excl, tf.ones(tf.shape(excl)[0], dtype=tf.bool), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    excl_d = tf.sparse.to_dense(tf.sparse.reorder(excl_sp))\n",
    "    incl_sp = tf.SparseTensor(incl, tf.ones(tf.shape(incl)[0], dtype=tf.bool), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    incl_d = tf.sparse.to_dense(tf.sparse.reorder(incl_sp))\n",
    "    mask_final = tf.logical_and(a_d, tf.logical_not(excl_d))\n",
    "    \n",
    "    true_fn = lambda: tf.logical_and(mask_final, incl_d)\n",
    "    mask_final = tf.cond(tf.greater(tf.shape(incl)[0], 0), true_fn, lambda: mask_final)\n",
    "    \n",
    "    neighbors = tf.where(mask_final)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_adjacent_proto(nj, siid, m, p, s, l):\n",
    "    \"\"\"Merge adjacent proto-clusters\"\"\"\n",
    "    sid, sidx = tf.split(s, num_or_size_splits=[1,3], axis=1)\n",
    "    lid, lidx = tf.split(l, num_or_size_splits=[1,3], axis=1)\n",
    "    proto_id, proto_idx = tf.split(p, num_or_size_splits=[1,3], axis=1)\n",
    "    nnj = neighbors(nj, excl=sidx)\n",
    "    nnj_sp = tf.SparseTensor(nnj, tf.ones(tf.shape(nnj)[0], tf.bool), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    nnj_d = tf.sparse.to_dense(tf.sparse.reorder(nnj_sp))\n",
    "    proto_sp = tf.SparseTensor(proto_idx, tf.reshape(proto_id, [-1,]), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    proto_d = tf.sparse.to_dense(tf.sparse.reorder(proto_sp))\n",
    "    indices = tf.where(tf.logical_and(tf.logical_and(nnj_d, tf.not_equal(proto_d, 0)), tf.greater(m, TNEIGHBOR)))\n",
    "#     indices = tf.where(tf.logical_and(nnj_d, tf.not_equal(proto_d, 0)))\n",
    "    values = tf.gather_nd(proto_d, indices)\n",
    "    values = tf.boolean_mask(values, tf.not_equal(values, siid))\n",
    "    \n",
    "    # loop over values\n",
    "    k0 = tf.constant(0)\n",
    "    ck = lambda k, p, s, l: tf.less(k, tf.shape(values)[0])\n",
    "    def bk(k, p, s, l):\n",
    "        neigh_id = tf.gather(values, k)\n",
    "        pnew = tf.where(tf.equal(proto_d, neigh_id), tf.reshape(siid, [-1,])*tf.ones_like(p), p)\n",
    "        snew = tf.where(tf.equal(sid, neigh_id), tf.reshape(siid, [-1,])*tf.ones_like(s), s)\n",
    "        lnew = tf.where(tf.equal(lid, neigh_id), tf.reshape(siid, [-1,])*tf.ones_like(l), l)\n",
    "        return [tf.add(k, 1), pnew, snew, lnew]\n",
    "    k, p, sid, lid = tf.while_loop(ck, bk, loop_vars=[k0, proto_d, sid, lid],\n",
    "                                   shape_invariants=[k0.get_shape(), proto_d.get_shape(),\n",
    "                                                     sid.get_shape(), lid.get_shape()])                \n",
    "    indices = tf.where(tf.not_equal(p, 0))\n",
    "    values = tf.expand_dims(tf.gather_nd(p, indices), axis=1)\n",
    "    pnew = tf.concat([values, indices], axis=1)\n",
    "    snew = tf.concat([sid, sidx], axis=1)\n",
    "    lnew = tf.concat([lid, lidx], axis=1)\n",
    "    return [pnew, snew, lnew]\n",
    "\n",
    "\n",
    "def bj_maker(j, m, p, s, l, n, siid):\n",
    "    \"\"\"Body of while loop over neighbors\"\"\"\n",
    "    nj = tf.gather(n, j)\n",
    "    nval = tf.gather_nd(m, nj)\n",
    "    nnew = tf.expand_dims(tf.concat([siid, nj], axis=0), axis=0)\n",
    "\n",
    "    def tneighbor(siid=siid):\n",
    "        \"\"\"Append cells to both proto-clusters and neighbor seed list if value above TNEIGHBOR\"\"\"\n",
    "        [pnew, snew, lnew] = merge_adjacent_proto(nj, siid, m, p, s, l)\n",
    "        pnew = tf.concat([pnew, nnew], axis=0)\n",
    "        lnew = tf.concat([lnew, nnew], axis=0)\n",
    "        return [pnew, snew, lnew]\n",
    "\n",
    "    def tcell():\n",
    "        \"\"\"Append cells to proto-clusters if value above TCELL\"\"\"\n",
    "        def true_fn():\n",
    "            return tf.concat([p, nnew], axis=0)\n",
    "        def false_fn(): return p\n",
    "        pnew = tf.cond(tf.greater(nval, TCELL), true_fn, false_fn)\n",
    "        return [pnew, s, l]\n",
    "    \n",
    "    [pnew, snew, lnew] = tf.cond(tf.greater(nval, TNEIGHBOR), true_fn=tneighbor, false_fn=tcell)\n",
    "    \n",
    "    return [tf.add(j, 1), m, pnew, snew, lnew, n, siid]\n",
    "    \n",
    "\n",
    "def bi_maker(i, m, p, s, l):\n",
    "    \"\"\"Body of while loop over current seed list\"\"\"\n",
    "    si = tf.gather(s, i)\n",
    "    \n",
    "    # find neighbours not in protolist\n",
    "    siid, siidx = tf.split(si, num_or_size_splits=[1,3], axis=0)\n",
    "    _, pidx = tf.split(p, num_or_size_splits=[1,3], axis=1)\n",
    "    n = neighbors(siidx, excl=pidx)\n",
    "        \n",
    "    # loop over neighbors\n",
    "    j0 = tf.constant(0)\n",
    "    cj = lambda j, m, p, s, l, n, siid: tf.less(j, tf.shape(n)[0])\n",
    "    _, m, p, s, l, _, _ = tf.while_loop(\n",
    "        cj, bj_maker, loop_vars=[j0, m, p, s, l, n, siid],\n",
    "        shape_invariants=[j0.get_shape(), m.get_shape(), tf.TensorShape([None, 4]),\n",
    "                          s.get_shape(), tf.TensorShape([None, 4]),\n",
    "                          n.get_shape(), siid.get_shape()])\n",
    "    \n",
    "    return [tf.add(i, 1), m, p, s, l]\n",
    "  \n",
    "def finding_neighbors_maker(m, p, s):\n",
    "    \"\"\"Finding neighbors recursively until current seed list is empty.\n",
    "    m: image, p: protolist, s: seedlist, l: neighlist, n: neighbor\"\"\"\n",
    "    slen = tf.shape(s)[0]\n",
    "    \n",
    "    # sort current seed list in descending order\n",
    "    s = sorting_seedlist(s, slen, m)\n",
    "    \n",
    "    # loop over current seed list\n",
    "    i0, l0 = tf.constant(0), tf.zeros([0,4], tf.int64)\n",
    "    ci = lambda i, m, p, s, l: tf.less(i, slen)\n",
    "    _, m, pnew, s, lnew = tf.while_loop(\n",
    "        ci, bi_maker, loop_vars=[i0, m, p, s, l0],\n",
    "        shape_invariants=[i0.get_shape(), m.get_shape(), tf.TensorShape([None, 4]),\n",
    "                          s.get_shape(), tf.TensorShape([None, 4])])\n",
    "    # neighbor seed list becomes the new seed list\n",
    "    snew = lnew\n",
    "        \n",
    "    # repeated until current seed list is empty\n",
    "    def true_fn(): return finding_neighbors_maker(m, pnew, snew)\n",
    "    def false_fn(): return pnew\n",
    "#     print(\"ciao\", tf.size(snew))\n",
    "    pnew = tf.cond(tf.not_equal(tf.size(snew), tf.constant(0)), true_fn, false_fn)\n",
    "    return pnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize cluster maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_proto(s):\n",
    "    sid, sidx = tf.split(s, num_or_size_splits=[1,3], axis=1)\n",
    "    u, uid = tf.unique(tf.reshape(sid, [-1]), out_idx=sid.dtype)\n",
    "    ulen = tf.shape(u)[0]\n",
    "    uid = tf.reshape(tf.add(uid, 1), [-1,1])\n",
    "    s = tf.concat([uid, sidx], axis=1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_maker(m, s):\n",
    "    \"\"\"Finalize cluster maker by removing clusters with energy \n",
    "    less than TENERGY and sort in descending order in energy\"\"\"\n",
    "    \n",
    "    # compute total cluster energy\n",
    "    sid, sidx = tf.split(s, num_or_size_splits=[1,3], axis=1)\n",
    "    u, uid = tf.unique(tf.reshape(sid, [-1]), out_idx=sid.dtype)\n",
    "    ulen = tf.shape(u)[0]\n",
    "    uid = tf.reshape(tf.add(uid, 1), [-1,1])\n",
    "    s = tf.concat([uid, sidx], axis=1)\n",
    "    \n",
    "    s_sp = tf.SparseTensor(sidx, tf.reshape(uid, [-1,]), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    s_dense = tf.sparse.to_dense(tf.sparse.reorder(s_sp))\n",
    "    i0 = tf.constant(1, s_dense.dtype)\n",
    "    E0 = tf.constant([], m.dtype)\n",
    "    c = lambda i, E: tf.less(i, tf.cast(ulen+1, s_dense.dtype))\n",
    "    def b(i, E):\n",
    "        indices = tf.where(tf.equal(s_dense, i))\n",
    "        values = tf.gather_nd(m, indices)\n",
    "        Ei = tf.expand_dims(tf.reduce_sum(values), axis=0)\n",
    "        return [tf.add(i, 1), tf.concat([E, Ei], axis=0)]\n",
    "    _, E = tf.while_loop(c, b, loop_vars=[i0, E0], shape_invariants=[i0.get_shape(), tf.TensorShape([None, 1])])\n",
    "#     print(\"total cluster energy\", E)\n",
    "    \n",
    "    # sort in descending order in energy and remove clusters with energy less than TENERGY\n",
    "    values, indices = tf.math.top_k(E, k=ulen)\n",
    "    indices = tf.cast(tf.add(indices, 1), s_dense.dtype)\n",
    "    indices = tf.boolean_mask(indices, tf.greater(values, TENERGY))\n",
    "    i0 = tf.constant(0)\n",
    "    p0 = tf.zeros_like(s_dense)\n",
    "    c = lambda i, p: tf.less(i, tf.shape(indices)[0])\n",
    "    def b(i, p):\n",
    "        ii = tf.gather(indices, i)\n",
    "        pnew = tf.add(p, tf.where(tf.equal(s_dense, ii), s_dense, tf.zeros_like(s_dense)))\n",
    "        return [tf.add(i, 1), pnew]\n",
    "    _, p = tf.while_loop(c, b, loop_vars=[i0, p0], shape_invariants=[i0.get_shape(), p0.get_shape()])\n",
    "    sidx = tf.where(tf.not_equal(p, 0))\n",
    "    sid = tf.expand_dims(tf.gather_nd(p, sidx), axis=1)\n",
    "    s = tf.concat([sid, sidx], axis=1)\n",
    "    \n",
    "    s = renumber_proto(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_maker(parsed):\n",
    "#     print('hello')\n",
    "    image = parsed['image']\n",
    "    \n",
    "    seedlist = finding_seeds(image)\n",
    "    seedlist = finding_neighbors_maker(image, seedlist, seedlist)\n",
    "    seedlist = finalize_maker(image, seedlist)\n",
    "    \n",
    "    parsed['proto'] = seedlist \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster maker is sufficient for isolated signals, but not for overlapping showers. If individual particles form local maxima they may still be separable. Acting on the clusters resulting from the cluster maker, this is what the cluster splitter does in the followning steps:\n",
    "\n",
    "* **Finding local maxima**: a local maxima is defined as a cell with: a) $E>t_\\text{locmax}$, b) energy greater than that of any neighboring cell, and c) number of neighboring cells withing the parent cluster $N>t_\\text{number}$ (default is $\\geq4$). Each local maximum forms a cluster and parent clusters without any local maximum cell will not be split.\n",
    "* **Finding neighbors**: the local maxima now becomes the initial seed list much like in cluster maker, except that only cells originally clustered are used, without thresholding and merging. Instead of merging, shared cells are added to a shared cell list to be handled separately.\n",
    "* **Shared cells**: the shared cell list is expanded iteratively adding neighbors from the originally clustered cells not yet assigned to any proto-cluster. Each of these are then added to the two adjoining proto-clusters with the weights $w_1=\\frac{E_{1}}{E_{1}+rE_{2}}, w_{2}=1-w_{1}, r=\\exp(d_{1}-d_{2})$, where $E_{1,2}$ are the energies of the two proto-clusters and $d_{1,2}$ are the distances of the shared cell to the proto-cluster centroids in units of a typical em shower scale.\n",
    "* **Finalize**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding local maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_local_maxima(m, p):\n",
    "    \"\"\"Find local maxima cells by looping over protolist.\n",
    "    m: filtered image, x: local maxima\"\"\"\n",
    "    \n",
    "    # loop over protolist\n",
    "    i0 = tf.constant(0)\n",
    "    x = tf.zeros([0, 4], p.dtype)\n",
    "    c = lambda i, p, x: tf.less(i, tf.shape(p)[0])\n",
    "    def b(i, p, x):\n",
    "        pi = tf.gather(p, i)\n",
    "        piid, piidx = tf.split(pi, num_or_size_splits=[1,3], axis=0)\n",
    "        pval = tf.gather_nd(m, piidx)\n",
    "        midx = tf.where(tf.greater(m, 0))\n",
    "        nidx = neighbors(piidx, incl=midx)\n",
    "        m_val = tf.gather_nd(m, nidx)\n",
    "#         m_val = tf.boolean_mask(m_val, tf.greater(m_val, 0))\n",
    "\n",
    "        pred = tf.logical_and(tf.logical_and(tf.greater(pval, TLOCMAX), \n",
    "                                      tf.greater(pval, tf.math.reduce_max(m_val))),\n",
    "                                      tf.greater(m_val.get_shape()[0], TNUMBER))\n",
    "        r = tf.cond(pred, lambda: tf.expand_dims(pi, axis=0), lambda: tf.zeros([0, 4], pi.dtype))\n",
    "        \n",
    "        return [tf.add(i, 1), p, tf.concat([x, r], axis=0)]\n",
    "    \n",
    "    _, p, x = tf.while_loop(\n",
    "        c, b, loop_vars=[i0, p, x], \n",
    "        shape_invariants=[i0.get_shape(), p.get_shape(), tf.TensorShape([None, 4])])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_shared(nj, p, siid):\n",
    "    nnj = neighbors(nj)\n",
    "    proto_id, proto_idx = tf.split(p, num_or_size_splits=[1,3], axis=1)\n",
    "    nnj_sp = tf.SparseTensor(nnj, tf.ones(tf.shape(nnj)[0], tf.bool), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    nnj_d = tf.sparse.to_dense(tf.sparse.reorder(nnj_sp))\n",
    "    proto_sp = tf.SparseTensor(proto_idx, tf.reshape(proto_id, [-1,]), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    proto_d = tf.sparse.to_dense(tf.sparse.reorder(proto_sp))\n",
    "    indices = tf.where(tf.logical_and(nnj_d, tf.not_equal(proto_d, 0)))\n",
    "    values = tf.gather_nd(proto_d, indices)\n",
    "    y, idx = tf.unique(values)\n",
    "    return [y, tf.equal(y.get_shape()[0], 2)]\n",
    "\n",
    "def bj_splitter(j, m, p, s, l, o, n, siid):\n",
    "    \"\"\"Body of while loop over neighbors\"\"\"\n",
    "    nj = tf.gather(n, j)\n",
    "    \n",
    "    ids, pred = is_shared(nj, p, siid)\n",
    "    \n",
    "    def true_fn():\n",
    "        nnew = tf.expand_dims(tf.concat([ids, nj], axis=0), axis=0)\n",
    "        onew = tf.concat([o, nnew], axis=0)\n",
    "        return [p, s, l, onew]\n",
    "    def false_fn():\n",
    "        nnew = tf.expand_dims(tf.concat([siid, nj], axis=0), axis=0)\n",
    "        pnew = tf.concat([p, nnew], axis=0)\n",
    "        lnew = tf.concat([l, nnew], axis=0)\n",
    "        return [pnew, s, lnew, o]\n",
    "\n",
    "    [pnew, snew, lnew, onew] = tf.cond(pred, true_fn, false_fn)\n",
    "        \n",
    "    return [tf.add(j, 1), m, pnew, snew, lnew, onew, n, siid]\n",
    "\n",
    "def bi_splitter(i, m, p, s, l, o):\n",
    "    \"\"\"Body of while loop over current seed list\"\"\"\n",
    "    si = tf.gather(s, i)\n",
    "    \n",
    "    # find neighbours not in protolist and only include cells originally clustered\n",
    "    siid, siidx = tf.split(si, num_or_size_splits=[1,3], axis=0)\n",
    "    _, pidx = tf.split(p, num_or_size_splits=[1,3], axis=1)\n",
    "    _, oidx = tf.split(o, num_or_size_splits=[2,3], axis=1)\n",
    "    midx = tf.where(tf.greater(m, 0))\n",
    "    \n",
    "    n = neighbors(siidx, excl=tf.concat([pidx, oidx], axis=0), incl=midx)\n",
    "    \n",
    "    # loop over neighbors\n",
    "    j0 = tf.constant(0)\n",
    "    cj = lambda j, m, p, s, l, o, n, siid: tf.less(j, n.get_shape()[0])\n",
    "    _, m, p, s, l, o, _, _ = tf.while_loop(\n",
    "        cj, bj_splitter, loop_vars=[j0, m, p, s, l, o, n, siid],\n",
    "        shape_invariants=[j0.get_shape(), m.get_shape(), tf.TensorShape([None, 4]),\n",
    "                          s.get_shape(), tf.TensorShape([None, 5]), tf.TensorShape([None, 4]),\n",
    "                          n.get_shape(), siid.get_shape()])\n",
    "    \n",
    "    return [tf.add(i, 1), m, p, s, l, o]\n",
    "\n",
    "def finding_neighbors_splitter(m, p, s, o):\n",
    "    \"\"\"Finding neighbors recursively until current seed list is empty.\n",
    "    m: image, p: protolist, s: seedlist, l: neighlist, n: neighbor, o: sharedlist\"\"\"\n",
    "    slen = tf.shape(s)[0]\n",
    "    \n",
    "    # sort current seed list in descending order\n",
    "    s = sorting_seedlist(s, slen, m)\n",
    "    \n",
    "    # loop over current seed list\n",
    "    i0, l0 = tf.constant(0), tf.zeros([0,4], tf.int64)\n",
    "    ci = lambda i, m, p, s, l, o: tf.less(i, slen)\n",
    "    _, m, pnew, s, lnew, onew = tf.while_loop(\n",
    "        ci, bi_splitter, loop_vars=[i0, m, p, s, l0, o],\n",
    "        shape_invariants=[i0.get_shape(), m.get_shape(), \n",
    "                          tf.TensorShape([None, 4]), s.get_shape(),\n",
    "                          tf.TensorShape([None, 4]), tf.TensorShape([None, 5])])\n",
    "    \n",
    "    # neighbor seed list becomes the new seed list\n",
    "    snew = lnew\n",
    "    \n",
    "    # repeated until current seed list is empty\n",
    "    def true_fn(): return finding_neighbors_splitter(m, pnew, snew, onew)\n",
    "    def false_fn(): return [pnew, onew]\n",
    "    [pnew, onew] = tf.cond(tf.not_equal(tf.size(snew), 0), true_fn, false_fn)\n",
    "    return [pnew, onew]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_shared(j, p, l, n, siid):\n",
    "    \"\"\"Body of while loop over neighbors\"\"\"\n",
    "    nj = tf.gather(n, j)\n",
    "    nnew = tf.expand_dims(tf.concat([siid, nj], axis=0), axis=0)\n",
    "    pnew = tf.concat([p, nnew], axis=0)\n",
    "    lnew = tf.concat([l, nnew], axis=0)\n",
    "    return [tf.add(j, 1), pnew, lnew, n, siid]\n",
    "\n",
    "def bi_shared(i, m, p, s, l):\n",
    "    \"\"\"Body of while loop over current seed list\"\"\"\n",
    "    si = tf.gather(s, i)\n",
    "    \n",
    "    # find neighbours not in protolist and only include cells originally clustered\n",
    "    siid, siidx = tf.split(si, num_or_size_splits=[2,3], axis=0)\n",
    "    _, pidx = tf.split(p, num_or_size_splits=[2,3], axis=1)\n",
    "    midx = tf.where(m)\n",
    "    \n",
    "    n = neighbors(siidx, excl=pidx, incl=midx)\n",
    "    \n",
    "    # loop over neighbors\n",
    "    j0 = tf.constant(0)\n",
    "    cj = lambda j, p, l, n, siid: tf.less(j, n.get_shape()[0])\n",
    "    _, p, l, _, _ = tf.while_loop(\n",
    "        cj, bj_shared, loop_vars=[j0, p, l, n, siid],\n",
    "        shape_invariants=[j0.get_shape(), tf.TensorShape([None, 5]), tf.TensorShape([None, 5]),\n",
    "                          n.get_shape(), siid.get_shape()])\n",
    "    \n",
    "    return [tf.add(i, 1), m, p, s, l]\n",
    "\n",
    "def expand_sharedlist(m, p, s):\n",
    "    # loop over current seed list\n",
    "    i0, l0 = tf.constant(0), tf.zeros([0,5], s.dtype)\n",
    "    ci = lambda i, m, p, s, l: tf.less(i, s.get_shape()[0])\n",
    "    _, _, pnew, s, lnew = tf.while_loop(\n",
    "        ci, bi_shared, loop_vars=[i0, m, p, s, l0],\n",
    "        shape_invariants=[i0.get_shape(), m.get_shape(), tf.TensorShape([None, 5]), \n",
    "                          s.get_shape(), tf.TensorShape([None, 5])])\n",
    "    \n",
    "    # neighbor seed list becomes the new seed list\n",
    "    snew = lnew\n",
    "    \n",
    "    # repeated until current seed list is empty\n",
    "    def true_fn(): return expand_sharedlist(m, pnew, snew)\n",
    "    def false_fn(): return pnew\n",
    "    slen = snew.get_shape()[0]\n",
    "    pnew = tf.cond(tf.greater(slen, 0), true_fn, false_fn)\n",
    "    return pnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize cluster splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_splitter(m, p, o):\n",
    "    pid, pidx = tf.split(p, num_or_size_splits=[1,3], axis=1)\n",
    "    pval = tf.gather_nd(m, pidx)\n",
    "    psp = tf.SparseTensor(pidx, tf.reshape(pid, [-1,]), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    pd = tf.sparse.to_dense(tf.sparse.reorder(psp))\n",
    "    pnew = tf.concat([tf.cast(pid, pval.dtype), tf.cast(pidx, pval.dtype), tf.expand_dims(pval, axis=1)], axis=1)\n",
    "    \n",
    "    y = tf.unique(tf.reshape(pid, [-1,]))[0]\n",
    "#     print(\"unique pid\", y)\n",
    "    \n",
    "    i0, com0, E0 = tf.constant(0), tf.zeros([0, 3], m.dtype), tf.zeros([0, 1], m.dtype)\n",
    "    ci = lambda i, com, E: tf.less(i, y.get_shape()[0])\n",
    "    def bi(i, com, E):\n",
    "        yi = tf.gather(y, i)\n",
    "        indices = tf.where(tf.equal(pd, yi))\n",
    "        values = tf.expand_dims(tf.gather_nd(m, indices), axis=1)\n",
    "        indices = tf.cast(indices, values.dtype)\n",
    "        comi = tf.reduce_sum(tf.multiply(indices, values), axis=0)\n",
    "        Ei = tf.reduce_sum(values)\n",
    "        comi = tf.divide(comi, Ei)\n",
    "        comi = tf.expand_dims(comi, axis=0)\n",
    "        Ei = tf.reshape(Ei, [-1,1])\n",
    "        return [tf.add(i, 1), tf.concat([com, comi], axis=0), tf.concat([E, Ei], axis=0)]\n",
    "    _, com, E = tf.while_loop(\n",
    "        ci, bi, loop_vars=[i0, com0, E0],\n",
    "        shape_invariants=[i0.get_shape(), tf.TensorShape([None, 3]), tf.TensorShape([None, 1])])\n",
    "#     print(com, E)\n",
    "    \n",
    "    j0, l0 = tf.constant(0), tf.zeros([0, 5], m.dtype)\n",
    "    cj = lambda j, l: tf.less(j, o.get_shape()[0])\n",
    "    def bj(j, l):\n",
    "        oj = tf.gather(o, j)\n",
    "        a, b, ojidx = tf.split(oj, num_or_size_splits=[1,1,3], axis=0)\n",
    "        oval = tf.expand_dims(tf.gather_nd(m, ojidx), axis=0)\n",
    "        ojidx = tf.cast(ojidx, com.dtype)\n",
    "        amask = tf.equal(y, a)\n",
    "        bmask = tf.equal(y, b)\n",
    "        acom = tf.reshape(tf.boolean_mask(com, amask), [-1,])\n",
    "        bcom = tf.reshape(tf.boolean_mask(com, bmask), [-1,])\n",
    "        d1 = tf.sqrt(tf.reduce_sum(tf.math.squared_difference(ojidx, acom))) / moliereRadius\n",
    "        d2 = tf.sqrt(tf.reduce_sum(tf.math.squared_difference(ojidx, bcom))) / moliereRadius\n",
    "        # missing in units of !\n",
    "        r = tf.exp(tf.subtract(d1, d2))\n",
    "        E1 = tf.boolean_mask(E, amask)\n",
    "        E2 = tf.boolean_mask(E, bmask)\n",
    "        w1 = tf.reshape(tf.divide(E1, tf.add(E1, tf.multiply(r, E2))), [-1,])\n",
    "        w2 = tf.subtract(1, w1)\n",
    "        la = tf.expand_dims(tf.concat([tf.cast(a, com.dtype), ojidx, tf.multiply(w1, oval)], axis=0), axis=0)\n",
    "        lb = tf.expand_dims(tf.concat([tf.cast(b, com.dtype), ojidx, tf.multiply(w2, oval)], axis=0), axis=0)\n",
    "        return [tf.add(j, 1), tf.concat([l, la, lb], axis=0)]\n",
    "    _, lnew = tf.while_loop(\n",
    "        cj, bj, loop_vars=[j0, l0],\n",
    "        shape_invariants=[j0.get_shape(), tf.TensorShape([None, 5])])\n",
    "\n",
    "    pnew = tf.concat([pnew, lnew], axis=0)\n",
    "\n",
    "    # sort in descending order in energy \n",
    "    indices = tf.reshape(tf.cast(tf.math.top_k(tf.reshape(E, [1, -1]), k=E.get_shape()[0])[1], o.dtype), [-1,])\n",
    "#     print(y, indices)\n",
    "    \n",
    "    k0, p0 = tf.constant(0), tf.zeros([0, 5], pnew.dtype)\n",
    "    ck = lambda k, p: tf.less(k, pnew.get_shape()[0])\n",
    "    def bk(k, p):\n",
    "        pk = tf.gather(pnew, k)\n",
    "        pkid, pkidx = tf.split(pk, num_or_size_splits=[1,4], axis=0)\n",
    "        yidx = tf.reshape(tf.where(tf.equal(y, tf.cast(pkid, y.dtype))), [-1,])\n",
    "        pkid = tf.cast(tf.add(tf.where(tf.equal(indices, yidx)), 1), pkidx.dtype)\n",
    "        pknew = tf.concat([pkid, tf.expand_dims(pkidx, axis=0)], axis=1)\n",
    "        return [tf.add(k, 1), tf.concat([p, pknew], axis=0)]\n",
    "    pnew = tf.while_loop(ck, bk, [k0, p0])[1]\n",
    "    \n",
    "    return pnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_splitter(parsed):    \n",
    "    proto = parsed['proto']\n",
    "    image = parsed['image']\n",
    "#     image = tf.constant(image)\n",
    "    \n",
    "    # filter image with protolist\n",
    "    proto_id, proto_idx = tf.split(proto, num_or_size_splits=[1,3], axis=1)\n",
    "    proto_val = tf.gather_nd(image, proto_idx)\n",
    "    image_sp = tf.SparseTensor(proto_idx, proto_val, dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    image_d = tf.sparse.to_dense(tf.sparse.reorder(image_sp))\n",
    "    \n",
    "    locmax = finding_local_maxima(image_d, proto)\n",
    "    \n",
    "    # create seed list from local maxima\n",
    "    _, locmax_idx = tf.split(locmax, num_or_size_splits=[1,3], axis=1)\n",
    "    proto_sp = tf.SparseTensor(proto_idx, tf.reshape(proto_id, [-1,]), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    proto_d = tf.sparse.to_dense(tf.sparse.reorder(proto_sp))\n",
    "    start = tf.add(tf.reduce_max(proto_id), 1)\n",
    "    limit = start + locmax.get_shape()[0]\n",
    "    locmax_id = tf.expand_dims(tf.range(start, limit, dtype=locmax.dtype), axis=1)\n",
    "    seedlist = tf.concat([locmax_id, locmax_idx], axis=1)\n",
    "    sharedlist = tf.zeros([0,5], tf.int64)\n",
    "    \n",
    "    [protolist, sharedlist] = finding_neighbors_splitter(image_d, seedlist, seedlist, sharedlist)\n",
    "    \n",
    "    # originally clustered cells not in protolist\n",
    "    protolist_id, protolist_idx = tf.split(protolist, num_or_size_splits=[1,3], axis=1)\n",
    "    protolist_sp = tf.SparseTensor(protolist_idx, tf.reshape(protolist_id, [-1,]), dense_shape=[HEIGHT, WIDTH, 2])\n",
    "    protolist_d = tf.sparse.to_dense(tf.sparse.reorder(protolist_sp))\n",
    "    mask = tf.logical_and(tf.cast(proto_d, tf.bool), tf.logical_not(tf.cast(protolist_d, tf.bool)))\n",
    "    \n",
    "    sharedlist = expand_sharedlist(mask, sharedlist, sharedlist)\n",
    "    \n",
    "    # add parent clusters without a local maximum\n",
    "    mask = tf.logical_and(tf.cast(proto_d, tf.bool), tf.cast(protolist_d, tf.bool))\n",
    "    proto_y = tf.expand_dims(tf.unique(tf.reshape(proto_id, [-1,]))[0], axis=0)\n",
    "    proto_masked_y = tf.expand_dims(tf.unique(tf.boolean_mask(proto_d, mask))[0], axis=0)\n",
    "    other = tf.sparse.to_dense(tf.sets.difference(proto_y, proto_masked_y))\n",
    "\n",
    "    other = tf.reshape(other, [-1,1])\n",
    "    \n",
    "    if tf.not_equal(tf.size(other), 0):\n",
    "        i = tf.constant(0)\n",
    "        c = lambda i, p: tf.less(i, tf.shape(other)[0])\n",
    "        def b(i, p): \n",
    "            otheri = tf.gather(other, i)\n",
    "            p = tf.where(tf.equal(proto_d, otheri), proto_d, p) \n",
    "            return [tf.add(i, 1), p]\n",
    "        protolist_d = tf.while_loop(c, b, [i, protolist_d])[1]\n",
    "        pidx = tf.where(protolist_d)\n",
    "        pid = tf.expand_dims(tf.gather_nd(protolist_d, pidx), axis=1)\n",
    "        protolist = tf.concat([pid, pidx], axis=1)\n",
    "    \n",
    "    cluster = finalize_splitter(image_d, protolist, sharedlist)\n",
    "        \n",
    "    parsed['cluster'] = cluster\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminating variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_features(parsed):\n",
    "    cluster = parsed['cluster']\n",
    "    \n",
    "    cid, cidx, cval = tf.split(cluster, num_or_size_splits=[1,3,1], axis=1)    \n",
    "    y = tf.unique(tf.reshape(cid, [-1,]))[0]\n",
    "#     print(\"unique pid\", y)\n",
    "    \n",
    "    i0 = tf.constant(0)\n",
    "    z0 = tf.zeros([0, 8], cluster.dtype)\n",
    "    c = lambda i, z: tf.less(i, tf.size(y))\n",
    "    def bi(i, z):\n",
    "        yi = tf.gather(y, i)\n",
    "        cid_indices = tf.where(tf.equal(tf.reshape(cid, [-1,]), yi))\n",
    "        values = tf.gather_nd(cval, cid_indices)\n",
    "        indices = tf.reshape(tf.gather(cidx, cid_indices), [-1,3])\n",
    "        cij, ck = tf.split(indices, num_or_size_splits=[2,1], axis=1)\n",
    "        com = tf.reduce_sum(tf.multiply(cij, values), axis=0, keepdims=True)/tf.reduce_sum(values)\n",
    "        \n",
    "        S_indices = tf.where(tf.equal(tf.reshape(ck, [-1,]), 0))\n",
    "        S_values = tf.gather_nd(values, S_indices)\n",
    "        S_cij = tf.reshape(tf.gather(cij, S_indices), [-1,2])\n",
    "        S_sum = tf.reduce_sum(S_values, keepdims=True)\n",
    "        S_max = tf.cond(tf.greater(S_sum, 0.), lambda: tf.reduce_max(S_values), lambda: tf.constant([[0.]]))\n",
    "        S_hot = tf.cond(tf.greater(S_sum, 0.), lambda: S_max/S_sum, lambda: tf.constant([[0.]]))\n",
    "        S_com = tf.reduce_sum(tf.multiply(S_cij, S_values), axis=0)/tf.reduce_sum(S_values)\n",
    "        S_rad = tf.sqrt(tf.reduce_sum(tf.pow(S_cij-S_com, 2), axis=1, keepdims=True))\n",
    "        S_rad_mean = tf.cond(tf.greater(S_sum, 0.), \n",
    "                             lambda: tf.reduce_sum(tf.multiply(S_rad, S_values), axis=0, keepdims=True)/tf.reduce_sum(S_values),\n",
    "                             lambda: tf.constant([[0.]]))\n",
    "        \n",
    "        C_indices = tf.where(tf.equal(tf.reshape(ck, [-1,]), 1))\n",
    "        C_values = tf.gather_nd(values, C_indices)\n",
    "        C_cij = tf.reshape(tf.gather(cij, C_indices), [-1,2])\n",
    "        C_sum = tf.reduce_sum(C_values, keepdims=True)\n",
    "        C_max = tf.cond(tf.greater(C_sum, 0.), lambda: tf.reduce_max(C_values), lambda: tf.constant([[0.]]))\n",
    "        C_hot = tf.cond(tf.greater(C_sum, 0.), lambda: C_max/C_sum, lambda: tf.constant([[0.]]))\n",
    "        C_com = tf.reduce_sum(tf.multiply(C_cij, C_values), axis=0)/tf.reduce_sum(C_values)\n",
    "        C_rad = tf.sqrt(tf.reduce_sum(tf.pow(C_cij-C_com, 2), axis=1, keepdims=True))\n",
    "        C_rad_mean = tf.cond(tf.greater(C_sum, 0.),\n",
    "                             lambda: tf.reduce_sum(tf.multiply(C_rad, C_values), axis=0, keepdims=True)/tf.reduce_sum(C_values),\n",
    "                             lambda: tf.constant([[0.]]))\n",
    "     \n",
    "        zi = tf.concat([com, S_sum, S_rad_mean, S_hot, C_sum, C_rad_mean, C_hot], axis=1)\n",
    "        \n",
    "        return [tf.add(i, 1), tf.concat([z, zi], axis=0)]\n",
    "    _, z = tf.while_loop(c, bi,\n",
    "                         loop_vars=[i0, z0],\n",
    "                         shape_invariants=[i0.get_shape(), tf.TensorShape([None, 8])])\n",
    "    parsed['scalar'] = z\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
