{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 284\n",
    "WIDTH = 284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'eventId': tf.io.FixedLenFeature([1], tf.int64, default_value=0),\n",
    "    'image': tf.io.FixedLenFeature([3], tf.string, default_value=[\"\",]*3)\n",
    "}\n",
    "\n",
    "feature_shape = {\n",
    "    'eventId': tf.TensorShape([1,]),\n",
    "    'image': tf.TensorShape([568, 568, 2])\n",
    "}\n",
    "\n",
    "def parser_fn(proto):\n",
    "    serialized = tf.io.parse_single_example(proto, feature_description)\n",
    "    deserialized = {k: (tf.sparse.to_dense(sparse_ops.deserialize_sparse(v, K.floatx()))\n",
    "                        if k != 'eventId' else v) for k,v in serialized.items()}\n",
    "    [deserialized[k].set_shape(feature_shape[k]) for k in deserialized.keys()]\n",
    "    x = deserialized['image']\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    x = K.pool2d(x, pool_size=(2, 2), strides=(2, 2), pool_mode='avg')\n",
    "    # sum instead of avg\n",
    "    x = 4.*x\n",
    "    deserialized['image'] = tf.squeeze(x)\n",
    "    \n",
    "    return deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ''\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 10\n",
    "\n",
    "filename = [os.path.join(DATA_DIR, 'B4.tfrecord')]\n",
    "dataset = tf.data.TFRecordDataset(filename, compression_type='GZIP', buffer_size=BUFFER_SIZE)\n",
    "dataset = dataset.map(parser_fn)\n",
    "# dataset = dataset.take(1).map(cluster_maker)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values\n",
    "TENERGY = 500\n",
    "TLOCMAX = 350\n",
    "TNUMBER = 3\n",
    "# good values to test if recursive function works\n",
    "TSEED = tf.constant(100, tf.float32)  # tf.constant([300.], tf.float32)\n",
    "TNEIGHBOR = tf.constant(50., tf.float32)\n",
    "TCELL = tf.constant(10., tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM 633\n",
    "TENERGY = 5000\n",
    "TLOCMAX = 500\n",
    "TNUMBER = 3\n",
    "# good values to test if recursive function works\n",
    "TSEED = tf.constant(6., tf.float32)  # tf.constant([300.], tf.float32)\n",
    "TNEIGHBOR = tf.constant(3., tf.float32)\n",
    "TCELL = tf.constant(3., tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Had 420\n",
    "# TENERGY = 0\n",
    "# TLOCMAX = 500\n",
    "# TNUMBER = 3\n",
    "# # good values to test if recursive function works\n",
    "# TSEED = tf.constant(4., tf.float32)  # tf.constant([300.], tf.float32)\n",
    "# TNEIGHBOR = tf.constant(2., tf.float32)\n",
    "# TCELL = tf.constant(0., tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['eventId', 'cluster_comi', 'cluster_comj', 'S_sum', 'S_rad_mean', 'S_hot', 'C_sum', 'C_rad_mean', 'C_hot']\n",
    "pdf = pd.DataFrame(columns=cols)\n",
    "i = 0\n",
    "for parsed in dataset.take(10):\n",
    "    print(i)\n",
    "    output = cluster_maker(parsed)\n",
    "    output2 = cluster_splitter(output)\n",
    "    output3 = scalar_features(output2)\n",
    "    s = parsed['scalar'].numpy()\n",
    "    eventId = parsed['eventId'].numpy()\n",
    "    eventId_rep = np.tile(eventId, [s.shape[0], 1])\n",
    "    arr = np.hstack((eventId_rep, s))\n",
    "    pdfi = pd.DataFrame(arr, columns=cols)\n",
    "    pdf = pdf.append(pdfi)\n",
    "    i += 1\n",
    "pdf.reset_index(drop=True, inplace=True)\n",
    "pdf.eventId = pdf.eventId.astype(int)\n",
    "pdf.to_csv('clustering.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(columns=['comi', 'comj', 'S_sum', 'S_rad_mean', 'S_hot', 'C_sum', 'S_rad_mean', 'C_hot'])\n",
    "fig, ax = plt.subplots(ncols=2, nrows=5, figsize=(10, 30))\n",
    "i = 0\n",
    "for parsed_record in dataset.take(1):\n",
    "    plt.imshow(parsed_record['image'][:,:,0])\n",
    "    plt.colorbar()\n",
    "#     print(parsed_record['labels'])\n",
    "#     output = cluster_maker(parsed_record)\n",
    "#     A = output['proto'].numpy()\n",
    "#     A1 = A[A[...,3] == 0][...,:3]\n",
    "#     A2 = A[A[...,3] == 1][...,:3]\n",
    "#     # A1 = np.unique(A1, axis=0)\n",
    "#     # A2 = np.unique(A2, axis=0)\n",
    "#     # print(A1)\n",
    "#     sp = tf.SparseTensor(A1[...,1:3], A1[...,0], dense_shape=[HEIGHT,WIDTH])\n",
    "#     A1_dense = tf.sparse.to_dense(tf.sparse.reorder(sp))\n",
    "#     sp = tf.SparseTensor(A2[...,1:3], A2[...,0], dense_shape=[HEIGHT,WIDTH])\n",
    "#     A2_dense = tf.sparse.to_dense(tf.sparse.reorder(sp))\n",
    "#     ax[i][0].imshow(A1_dense)\n",
    "#     ax[i][1].imshow(A2_dense)\n",
    "    \n",
    "#     output2 = cluster_splitter(output)\n",
    "# #     A = output['cluster'].numpy()\n",
    "# #     A1 = A[A[...,3] == 0][...,:3]\n",
    "# #     A2 = A[A[...,3] == 1][...,:3]\n",
    "# # #     # A1 = np.unique(A1, axis=0)\n",
    "# # #     # A2 = np.unique(A2, axis=0)\n",
    "\n",
    "#     output3 = scalar_features(output2)\n",
    "#     print(parsed_record['scalar'])\n",
    "# #     u = np.unique(A1[:,0])\n",
    "# #     print(u)\n",
    "    \n",
    "# #     sp = tf.SparseTensor(A1[...,1:3], A1[...,0], dense_shape=[HEIGHT,WIDTH])\n",
    "# #     A1_dense = tf.sparse.to_dense(tf.sparse.reorder(sp))\n",
    "# #     ax[i][1].imshow(A1_dense)\n",
    "    \n",
    "#     i += 1\n",
    "#     print(\"eventId\", parsed_record['eventId'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
